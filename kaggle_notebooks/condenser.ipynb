{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5b2f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:56:37.623243Z",
     "iopub.status.busy": "2023-10-04T10:56:37.622946Z",
     "iopub.status.idle": "2023-10-04T10:56:41.022493Z",
     "shell.execute_reply": "2023-10-04T10:56:41.021445Z"
    },
    "papermill": {
     "duration": 3.405984,
     "end_time": "2023-10-04T10:56:41.024974",
     "exception": false,
     "start_time": "2023-10-04T10:56:37.618990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'zalo2021'...\r\n",
      "remote: Enumerating objects: 2272, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (309/309), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (259/259), done.\u001b[K\r\n",
      "remote: Total 2272 (delta 113), reused 196 (delta 46), pack-reused 1963\u001b[K\r\n",
      "Receiving objects: 100% (2272/2272), 10.54 MiB | 19.79 MiB/s, done.\r\n",
      "Resolving deltas: 100% (642/642), done.\r\n",
      "/kaggle/working/zalo2021\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/coangquang/zalo2021.git\n",
    "%cd zalo2021\n",
    "!mkdir generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3baa6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:56:41.034454Z",
     "iopub.status.busy": "2023-10-04T10:56:41.034149Z",
     "iopub.status.idle": "2023-10-04T10:56:41.038336Z",
     "shell.execute_reply": "2023-10-04T10:56:41.037393Z"
    },
    "papermill": {
     "duration": 0.011201,
     "end_time": "2023-10-04T10:56:41.040337",
     "exception": false,
     "start_time": "2023-10-04T10:56:41.029136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "#!pip install transformers==4.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7ace84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:56:41.046825Z",
     "iopub.status.busy": "2023-10-04T10:56:41.046586Z",
     "iopub.status.idle": "2023-10-04T10:56:43.622133Z",
     "shell.execute_reply": "2023-10-04T10:56:43.621121Z"
    },
    "papermill": {
     "duration": 2.580745,
     "end_time": "2023-10-04T10:56:43.623870",
     "exception": false,
     "start_time": "2023-10-04T10:56:41.043125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "my_secret = user_secrets.get_secret(\"wandb_api_key\")\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccecd757",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:56:43.631212Z",
     "iopub.status.busy": "2023-10-04T10:56:43.630766Z",
     "iopub.status.idle": "2023-10-04T10:56:43.634808Z",
     "shell.execute_reply": "2023-10-04T10:56:43.633909Z"
    },
    "papermill": {
     "duration": 0.009549,
     "end_time": "2023-10-04T10:56:43.636579",
     "exception": false,
     "start_time": "2023-10-04T10:56:43.627030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python create_corpus.py --data_dir /kaggle/input/ltr2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682a6793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:56:43.643616Z",
     "iopub.status.busy": "2023-10-04T10:56:43.643365Z",
     "iopub.status.idle": "2023-10-04T10:56:44.638752Z",
     "shell.execute_reply": "2023-10-04T10:56:44.637414Z"
    },
    "papermill": {
     "duration": 1.001495,
     "end_time": "2023-10-04T10:56:44.641099",
     "exception": false,
     "start_time": "2023-10-04T10:56:43.639604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir generated_data/condenser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba139aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:56:44.649287Z",
     "iopub.status.busy": "2023-10-04T10:56:44.648420Z",
     "iopub.status.idle": "2023-10-04T10:58:20.914948Z",
     "shell.execute_reply": "2023-10-04T10:58:20.913732Z"
    },
    "papermill": {
     "duration": 96.273232,
     "end_time": "2023-10-04T10:58:20.917491",
     "exception": false,
     "start_time": "2023-10-04T10:56:44.644259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 678/678 [00:00<00:00, 4.42MB/s]\r\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███| 895k/895k [00:00<00:00, 14.9MB/s]\r\n",
      "Downloading (…)solve/main/bpe.codes: 100%|█| 1.14M/1.14M [00:00<00:00, 45.5MB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "100%|████████████████████████████████████| 60795/60795 [01:22<00:00, 738.49it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python Condenser/helper/create_train.py --tokenizer_name vinai/phobert-base-v2 --file /kaggle/input/retraining/retrain.json --save_to generated_data/condenser --max_len 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259dfa63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:58:20.936285Z",
     "iopub.status.busy": "2023-10-04T10:58:20.935979Z",
     "iopub.status.idle": "2023-10-04T10:58:21.988798Z",
     "shell.execute_reply": "2023-10-04T10:58:21.987551Z"
    },
    "papermill": {
     "duration": 1.064705,
     "end_time": "2023-10-04T10:58:21.991111",
     "exception": false,
     "start_time": "2023-10-04T10:58:20.926406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9022eff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T10:58:22.011067Z",
     "iopub.status.busy": "2023-10-04T10:58:22.010183Z",
     "iopub.status.idle": "2023-10-04T21:59:05.423128Z",
     "shell.execute_reply": "2023-10-04T21:59:05.421932Z"
    },
    "papermill": {
     "duration": 39643.425234,
     "end_time": "2023-10-04T21:59:05.425655",
     "exception": false,
     "start_time": "2023-10-04T10:58:22.000421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "Parse else mark\r\n",
      "Parse done\r\n",
      "Before train dataset ['generated_data/condenser/condenser_corpus_vectors.json']\r\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-6294bb6a9a353517/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\r\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6132.02it/s]\r\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1056.50it/s]\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-6294bb6a9a353517/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\r\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 135.31it/s]\r\n",
      "Before validate dataset None\r\n",
      "[INFO|configuration_utils.py:713] 2023-10-04 10:58:36,741 >> loading configuration file /kaggle/input/phobert-v2-9/config.json\r\n",
      "[INFO|configuration_utils.py:775] 2023-10-04 10:58:36,742 >> Model config RobertaConfig {\r\n",
      "  \"_name_or_path\": \"/kaggle/input/phobert-v2-9\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"RobertaForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classifier_dropout\": null,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-05,\r\n",
      "  \"max_position_embeddings\": 258,\r\n",
      "  \"model_type\": \"roberta\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.33.0\",\r\n",
      "  \"type_vocab_size\": 1,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 64001\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_auto.py:535] 2023-10-04 10:58:36,743 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\r\n",
      "[INFO|configuration_utils.py:713] 2023-10-04 10:58:36,744 >> loading configuration file /kaggle/input/phobert-v2-9/config.json\r\n",
      "[INFO|configuration_utils.py:775] 2023-10-04 10:58:36,744 >> Model config RobertaConfig {\r\n",
      "  \"_name_or_path\": \"/kaggle/input/phobert-v2-9\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"RobertaForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classifier_dropout\": null,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-05,\r\n",
      "  \"max_position_embeddings\": 258,\r\n",
      "  \"model_type\": \"roberta\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.33.0\",\r\n",
      "  \"type_vocab_size\": 1,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 64001\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-10-04 10:58:36,750 >> loading file vocab.txt\r\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-10-04 10:58:36,750 >> loading file bpe.codes\r\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-10-04 10:58:36,750 >> loading file added_tokens.json\r\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-10-04 10:58:36,750 >> loading file special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1850] 2023-10-04 10:58:36,751 >> loading file tokenizer_config.json\r\n",
      "[INFO|configuration_utils.py:713] 2023-10-04 10:58:36,751 >> loading configuration file /kaggle/input/phobert-v2-9/config.json\r\n",
      "[INFO|configuration_utils.py:775] 2023-10-04 10:58:36,752 >> Model config RobertaConfig {\r\n",
      "  \"_name_or_path\": \"/kaggle/input/phobert-v2-9\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"RobertaForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"bos_token_id\": 0,\r\n",
      "  \"classifier_dropout\": null,\r\n",
      "  \"eos_token_id\": 2,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-05,\r\n",
      "  \"max_position_embeddings\": 258,\r\n",
      "  \"model_type\": \"roberta\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 1,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.33.0\",\r\n",
      "  \"type_vocab_size\": 1,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 64001\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils.py:426] 2023-10-04 10:58:36,893 >> Adding <mask> to the vocabulary\r\n",
      "[WARNING|logging.py:290] 2023-10-04 10:58:36,893 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "[INFO|modeling_utils.py:2854] 2023-10-04 10:58:36,950 >> loading weights file /kaggle/input/phobert-v2-9/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:3643] 2023-10-04 10:58:41,818 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:3651] 2023-10-04 10:58:41,819 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /kaggle/input/phobert-v2-9.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\r\n",
      "[INFO|trainer.py:1712] 2023-10-04 10:58:47,877 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:1713] 2023-10-04 10:58:47,878 >>   Num examples = 94,690\r\n",
      "[INFO|trainer.py:1714] 2023-10-04 10:58:47,878 >>   Num Epochs = 8\r\n",
      "[INFO|trainer.py:1715] 2023-10-04 10:58:47,878 >>   Instantaneous batch size per device = 16\r\n",
      "[INFO|trainer.py:1717] 2023-10-04 10:58:47,878 >>   Training with DataParallel so batch size has been adjusted to: 32\r\n",
      "[INFO|trainer.py:1718] 2023-10-04 10:58:47,878 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\r\n",
      "[INFO|trainer.py:1719] 2023-10-04 10:58:47,878 >>   Gradient Accumulation steps = 4\r\n",
      "[INFO|trainer.py:1720] 2023-10-04 10:58:47,878 >>   Total optimization steps = 5,920\r\n",
      "[INFO|trainer.py:1721] 2023-10-04 10:58:47,879 >>   Number of trainable parameters = 149,239,553\r\n",
      "[INFO|integration_utils.py:716] 2023-10-04 10:58:47,883 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mheliolucas2306\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.11 is available!  To upgrade, please run:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/zalo2021/wandb/run-20231004_105848-7t7b8ihl\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfaithful-dawn-28\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/heliolucas2306/huggingface\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/heliolucas2306/huggingface/runs/7t7b8ihl\u001b[0m\r\n",
      "  0%|                                                  | 0/5920 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 4.0072, 'learning_rate': 4.222972972972973e-05, 'epoch': 0.68}\r\n",
      "{'loss': 2.5579, 'learning_rate': 4.617117117117117e-05, 'epoch': 1.35}\r\n",
      " 17%|█████▉                             | 1000/5920 [1:51:25<9:08:45,  6.69s/it][INFO|configuration_utils.py:460] 2023-10-04 12:50:44,126 >> Configuration saved in saved_model/checkpoint-1000/config.json\r\n",
      "[INFO|modeling_utils.py:1992] 2023-10-04 12:50:45,284 >> Model weights saved in saved_model/checkpoint-1000/pytorch_model.bin\r\n",
      "/kaggle/working/zalo2021/Condenser/modeling.py:127: UserWarning: omiting 204 transformer weights\r\n",
      "  warnings.warn(f'omiting {len(hf_weight_keys)} transformer weights')\r\n",
      "[INFO|tokenization_utils_base.py:2235] 2023-10-04 12:50:45,416 >> tokenizer config file saved in saved_model/checkpoint-1000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2242] 2023-10-04 12:50:45,417 >> Special tokens file saved in saved_model/checkpoint-1000/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2292] 2023-10-04 12:50:45,417 >> added tokens file saved in saved_model/checkpoint-1000/added_tokens.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 2.2907, 'learning_rate': 4.147897897897898e-05, 'epoch': 2.03}\r\n",
      "{'loss': 2.1503, 'learning_rate': 3.678678678678679e-05, 'epoch': 2.7}\r\n",
      " 34%|███████████▊                       | 2000/5920 [3:42:46<7:16:12,  6.68s/it][INFO|configuration_utils.py:460] 2023-10-04 14:42:05,348 >> Configuration saved in saved_model/checkpoint-2000/config.json\r\n",
      "[INFO|modeling_utils.py:1992] 2023-10-04 14:42:06,854 >> Model weights saved in saved_model/checkpoint-2000/pytorch_model.bin\r\n",
      "/kaggle/working/zalo2021/Condenser/modeling.py:127: UserWarning: omiting 204 transformer weights\r\n",
      "  warnings.warn(f'omiting {len(hf_weight_keys)} transformer weights')\r\n",
      "[INFO|tokenization_utils_base.py:2235] 2023-10-04 14:42:06,985 >> tokenizer config file saved in saved_model/checkpoint-2000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2242] 2023-10-04 14:42:06,985 >> Special tokens file saved in saved_model/checkpoint-2000/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2292] 2023-10-04 14:42:06,986 >> added tokens file saved in saved_model/checkpoint-2000/added_tokens.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 2.0537, 'learning_rate': 3.20945945945946e-05, 'epoch': 3.38}\r\n",
      "{'loss': 1.9923, 'learning_rate': 2.7402402402402405e-05, 'epoch': 4.05}\r\n",
      " 51%|█████████████████▋                 | 3000/5920 [5:34:05<5:24:14,  6.66s/it][INFO|configuration_utils.py:460] 2023-10-04 16:33:24,235 >> Configuration saved in saved_model/checkpoint-3000/config.json\r\n",
      "[INFO|modeling_utils.py:1992] 2023-10-04 16:33:25,434 >> Model weights saved in saved_model/checkpoint-3000/pytorch_model.bin\r\n",
      "/kaggle/working/zalo2021/Condenser/modeling.py:127: UserWarning: omiting 204 transformer weights\r\n",
      "  warnings.warn(f'omiting {len(hf_weight_keys)} transformer weights')\r\n",
      "[INFO|tokenization_utils_base.py:2235] 2023-10-04 16:33:25,563 >> tokenizer config file saved in saved_model/checkpoint-3000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2242] 2023-10-04 16:33:25,563 >> Special tokens file saved in saved_model/checkpoint-3000/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2292] 2023-10-04 16:33:25,564 >> added tokens file saved in saved_model/checkpoint-3000/added_tokens.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 1.9331, 'learning_rate': 2.2710210210210212e-05, 'epoch': 4.73}\r\n",
      "{'loss': 1.8892, 'learning_rate': 1.801801801801802e-05, 'epoch': 5.41}\r\n",
      " 68%|███████████████████████▋           | 4000/5920 [7:25:35<3:33:55,  6.68s/it][INFO|configuration_utils.py:460] 2023-10-04 18:24:53,812 >> Configuration saved in saved_model/checkpoint-4000/config.json\r\n",
      "[INFO|modeling_utils.py:1992] 2023-10-04 18:24:55,008 >> Model weights saved in saved_model/checkpoint-4000/pytorch_model.bin\r\n",
      "/kaggle/working/zalo2021/Condenser/modeling.py:127: UserWarning: omiting 204 transformer weights\r\n",
      "  warnings.warn(f'omiting {len(hf_weight_keys)} transformer weights')\r\n",
      "[INFO|tokenization_utils_base.py:2235] 2023-10-04 18:24:55,140 >> tokenizer config file saved in saved_model/checkpoint-4000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2242] 2023-10-04 18:24:55,140 >> Special tokens file saved in saved_model/checkpoint-4000/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2292] 2023-10-04 18:24:55,141 >> added tokens file saved in saved_model/checkpoint-4000/added_tokens.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 1.8551, 'learning_rate': 1.3325825825825828e-05, 'epoch': 6.08}\r\n",
      "{'loss': 1.8304, 'learning_rate': 8.633633633633633e-06, 'epoch': 6.76}\r\n",
      " 84%|█████████████████████████████▌     | 5000/5920 [9:17:05<1:42:07,  6.66s/it][INFO|configuration_utils.py:460] 2023-10-04 20:16:23,443 >> Configuration saved in saved_model/checkpoint-5000/config.json\r\n",
      "[INFO|modeling_utils.py:1992] 2023-10-04 20:16:24,811 >> Model weights saved in saved_model/checkpoint-5000/pytorch_model.bin\r\n",
      "/kaggle/working/zalo2021/Condenser/modeling.py:127: UserWarning: omiting 204 transformer weights\r\n",
      "  warnings.warn(f'omiting {len(hf_weight_keys)} transformer weights')\r\n",
      "[INFO|tokenization_utils_base.py:2235] 2023-10-04 20:16:24,952 >> tokenizer config file saved in saved_model/checkpoint-5000/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2242] 2023-10-04 20:16:24,953 >> Special tokens file saved in saved_model/checkpoint-5000/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2292] 2023-10-04 20:16:24,953 >> added tokens file saved in saved_model/checkpoint-5000/added_tokens.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\r\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\r\n",
      "{'loss': 1.8068, 'learning_rate': 3.941441441441441e-06, 'epoch': 7.43}\r\n",
      "100%|████████████████████████████████████| 5920/5920 [10:59:41<00:00,  6.24s/it][INFO|trainer.py:1960] 2023-10-04 21:58:59,988 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 39612.1103, 'train_samples_per_second': 19.123, 'train_steps_per_second': 0.149, 'train_loss': 2.185284258868243, 'epoch': 8.0}\r\n",
      "100%|████████████████████████████████████| 5920/5920 [10:59:41<00:00,  6.69s/it]\r\n",
      "[INFO|configuration_utils.py:460] 2023-10-04 21:58:59,996 >> Configuration saved in saved_model/config.json\r\n",
      "[INFO|modeling_utils.py:1992] 2023-10-04 21:59:01,204 >> Model weights saved in saved_model/pytorch_model.bin\r\n",
      "/kaggle/working/zalo2021/Condenser/modeling.py:127: UserWarning: omiting 204 transformer weights\r\n",
      "  warnings.warn(f'omiting {len(hf_weight_keys)} transformer weights')\r\n",
      "[INFO|tokenization_utils_base.py:2235] 2023-10-04 21:59:01,380 >> tokenizer config file saved in saved_model/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2242] 2023-10-04 21:59:01,381 >> Special tokens file saved in saved_model/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:2292] 2023-10-04 21:59:01,381 >> added tokens file saved in saved_model/added_tokens.json\r\n"
     ]
    }
   ],
   "source": [
    "!python Condenser/run_pre_training.py \\\n",
    "  --output_dir saved_model \\\n",
    "  --model_name_or_path /kaggle/input/phobert-v2-9 \\\n",
    "  --do_train \\\n",
    "  --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --fp16 \\\n",
    "  --warmup_ratio 0.1 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --num_train_epochs 8 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --dataloader_num_workers 2\\\n",
    "  --n_head_layers 2 \\\n",
    "  --skip_from 6 \\\n",
    "  --max_seq_length 256 \\\n",
    "  --train_dir generated_data/condenser \\\n",
    "  --weight_decay 0.01 \\\n",
    "  --late_mlm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39754.055811,
   "end_time": "2023-10-04T21:59:08.490886",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-04T10:56:34.435075",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
