{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets==2.11\n!pip install faiss-cpu\n!pip install pyvi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-19T16:17:46.657332Z","iopub.execute_input":"2023-10-19T16:17:46.657602Z","iopub.status.idle":"2023-10-19T16:18:18.612573Z","shell.execute_reply.started":"2023-10-19T16:17:46.657577Z","shell.execute_reply":"2023-10-19T16:18:18.611461Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting datasets==2.11\n  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (11.0.0)\nCollecting dill<0.3.7,>=0.3.0 (from datasets==2.11)\n  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (0.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.11) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.11) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.11) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.11) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.11) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.11) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.11) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.11) (2023.7.22)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.11)\n  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.11) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.11) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.11) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.11) (1.16.0)\nInstalling collected packages: dill, multiprocess, datasets\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.15\n    Uninstalling multiprocess-0.70.15:\n      Successfully uninstalled multiprocess-0.70.15\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.11.0 dill-0.3.6 multiprocess-0.70.14\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.7.4\nCollecting pyvi\n  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pyvi) (1.2.2)\nCollecting sklearn-crfsuite (from pyvi)\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.23.5)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.11.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.1.0)\nCollecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (1.16.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (4.66.1)\nInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\nSuccessfully installed python-crfsuite-0.9.9 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/coangquang/zalo2021.git\n%cd zalo2021\n!mkdir outputs\n!mkdir outputs/model\n!mkdir outputs/index\n!mkdir outputs/data\n!mkdir outputs/data/final\n!mkdir outputs/data/best","metadata":{"execution":{"iopub.status.busy":"2023-10-19T16:18:18.615075Z","iopub.execute_input":"2023-10-19T16:18:18.615527Z","iopub.status.idle":"2023-10-19T16:18:27.133745Z","shell.execute_reply.started":"2023-10-19T16:18:18.615490Z","shell.execute_reply":"2023-10-19T16:18:27.132617Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'zalo2021'...\nremote: Enumerating objects: 2441, done.\u001b[K\nremote: Counting objects: 100% (197/197), done.\u001b[K\nremote: Compressing objects: 100% (112/112), done.\u001b[K\nremote: Total 2441 (delta 130), reused 123 (delta 85), pack-reused 2244\u001b[K\nReceiving objects: 100% (2441/2441), 10.49 MiB | 20.70 MiB/s, done.\nResolving deltas: 100% (855/855), done.\n/kaggle/working/zalo2021\n","output_type":"stream"}]},{"cell_type":"code","source":"!python  dpr/train_dpr.py   \\\n    --corpus_file /kaggle/input/processed/corpus.csv   \\\n    --data_dir /kaggle/input/stage1-mlm-15   \\\n    --BE_checkpoint vinai/phobert-base-v2   \\\n    --BE_representation 0   \\\n    --BE_score dot   \\\n    --BE_num_epochs 15   \\\n    --q_len 32   \\\n    --ctx_len 256   \\\n    --BE_train_batch_size 256   \\\n    --BE_val_batch_size 384   \\\n    --q_chunk_size 64   \\\n    --ctx_chunk_size 32   \\\n    --BE_lr 0.00001   \\\n    --gradient_accumulation_steps 1  \\\n    --BE_loss 1.0   \\\n    --no_hard 3  \\\n    --patience 10  \\\n    --final_path outputs/model/final.pth.tar  \\\n    --biencoder_path outputs/model/best.pth.tar   \\\n    --grad_cache True   \\\n    --load_path /kaggle/input/stage1-mlm-15/best.pth.tar","metadata":{"execution":{"iopub.status.busy":"2023-10-19T16:18:27.135215Z","iopub.execute_input":"2023-10-19T16:18:27.135497Z","iopub.status.idle":"2023-10-19T17:04:43.538455Z","shell.execute_reply.started":"2023-10-19T16:18:27.135473Z","shell.execute_reply":"2023-10-19T17:04:43.537424Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nDownloading (…)lve/main/config.json: 100%|█████| 678/678 [00:00<00:00, 3.70MB/s]\nDownloading (…)solve/main/vocab.txt: 100%|███| 895k/895k [00:00<00:00, 10.4MB/s]\nDownloading (…)solve/main/bpe.codes: 100%|█| 1.14M/1.14M [00:00<00:00, 25.9MB/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\t* Loading data...\nNamespace(corpus_file='/kaggle/input/processed/corpus.csv', data_dir='/kaggle/input/processed', BE_checkpoint='vinai/phobert-base-v2', BE_representation=0, BE_score='dot', load_path=None, q_fixed=False, ctx_fixed=False, BE_num_epochs=1, grad_cache=True, q_len=32, ctx_len=256, BE_train_batch_size=256, BE_val_batch_size=384, q_chunk_size=64, ctx_chunk_size=32, BE_lr=1e-05, gradient_accumulation_steps=1, BE_loss=1.0, no_hard=0, patience=10, final_path='outputs/model/final.pth.tar', biencoder_path='outputs/model/best.pth.tar', index_path=None)\nNo of GPU(s): 2\nDownloading pytorch_model.bin: 100%|██████████| 540M/540M [00:02<00:00, 215MB/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nParallel Training\n\n ==================== Validation before training ====================\n100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.19s/it]\n-> Valid. time: 8.1862s, loss: 2.3499, accuracy: 49.3075%\n\n\n ==================== Training biencoder model on device: cuda ====================\n* Training epoch 1:\nAvg. batch proc. time: 10.6874s, loss: 2.3297: 100%|█| 10/10 [01:46<00:00, 10.69\n-> Training time: 106.9213s, loss = 2.3297, accuracy: 46.6371%\n* Validation for epoch 1:\n100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.66s/it]\n-> Valid. time: 2.6633s, loss: 1.1595, accuracy: 68.9751%\n\nCheck with the final state:\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7404de7aadb6d79d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\nDownloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4934.48it/s]\nExtracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 223.28it/s]\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7404de7aadb6d79d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n100%|██████████████████████████████████████████| 61/61 [00:00<00:00, 120.78it/s]\n1122.75950050354\nTesting hit scores with top_1:\n\tVal hit acc: 32.2857%\n\tVal all acc: 31.7143%\n\tTest hit acc: 29.8206%\n\tTest all acc: 29.3722%\nTesting hit scores with top_5:\n\tVal hit acc: 54.5714%\n\tVal all acc: 53.1429%\n\tTest hit acc: 52.9148%\n\tTest all acc: 51.7937%\nTesting hit scores with top_10:\n\tVal hit acc: 63.1429%\n\tVal all acc: 61.7143%\n\tTest hit acc: 61.4350%\n\tTest all acc: 60.5381%\nTesting hit scores with top_30:\n\tVal hit acc: 75.4286%\n\tVal all acc: 74.5714%\n\tTest hit acc: 76.0090%\n\tTest all acc: 75.1121%\nTesting hit scores with top_100:\n\tVal hit acc: 89.1429%\n\tVal all acc: 88.2857%\n\tTest hit acc: 87.2197%\n\tTest all acc: 86.5471%\nCheck with the best state:\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSome weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████████████████████████████████████| 61/61 [00:00<00:00, 136.09it/s]\n1121.2889695167542\nTesting hit scores with top_1:\n\tVal hit acc: 32.2857%\n\tVal all acc: 31.7143%\n\tTest hit acc: 29.8206%\n\tTest all acc: 29.3722%\nTesting hit scores with top_5:\n\tVal hit acc: 54.5714%\n\tVal all acc: 53.1429%\n\tTest hit acc: 52.9148%\n\tTest all acc: 51.7937%\nTesting hit scores with top_10:\n\tVal hit acc: 63.1429%\n\tVal all acc: 61.7143%\n\tTest hit acc: 61.4350%\n\tTest all acc: 60.5381%\nTesting hit scores with top_30:\n\tVal hit acc: 75.4286%\n\tVal all acc: 74.5714%\n\tTest hit acc: 76.0090%\n\tTest all acc: 75.1121%\nTesting hit scores with top_100:\n\tVal hit acc: 89.1429%\n\tVal all acc: 88.2857%\n\tTest hit acc: 87.2197%\n\tTest all acc: 86.5471%\n","output_type":"stream"}]}]}